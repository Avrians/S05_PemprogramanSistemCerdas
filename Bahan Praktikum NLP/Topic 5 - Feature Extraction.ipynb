{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"MCpA1ss3YBHM"},"source":["# Bag of Words-Basic\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rT4-6oXNVMgo"},"source":["## Import libraries"]},{"cell_type":"markdown","metadata":{"id":"vVmmHkYaYE0L"},"source":["Dwi Intan Af'idah"]},{"cell_type":"code","metadata":{"id":"0kx7mrKMYBHQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634870042606,"user_tz":-420,"elapsed":2142,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"b7845d31-82dd-42a3-84e6-886163428550"},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer \n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","import pandas as pd\n","import re\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}]},{"cell_type":"markdown","metadata":{"id":"3h7-dUkUYBHS"},"source":["## Take in a list of sentences"]},{"cell_type":"code","metadata":{"id":"XtDXBzn-YBHS"},"source":["sentences = [\"We are reading about Natural Language Processing Here\",\n","            \"Natural Language Processing making computers comprehend language data\",\n","            \"The field of Natural Language Processing is evolving everyday\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzGk1t_zYBHT"},"source":["## Create a Pandas Series of the object"]},{"cell_type":"code","metadata":{"id":"29sZzeiEYBHT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634870062095,"user_tz":-420,"elapsed":10,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"1cedab44-ec67-4297-9488-36acf971aa99"},"source":["corpus = pd.Series(sentences)\n","corpus"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    We are reading about Natural Language Processi...\n","1    Natural Language Processing making computers c...\n","2    The field of Natural Language Processing is ev...\n","dtype: object"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"_-l3WhR_YBHU"},"source":["## Data preprocessing"]},{"cell_type":"code","metadata":{"id":"pGQzjmh0YBHV"},"source":["def text_clean(corpus, keep_list):\n","    '''\n","    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n","    \n","    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n","            even after the cleaning process\n","    \n","    Output : Returns the cleaned text corpus\n","    \n","    '''\n","    cleaned_corpus = pd.Series()\n","    for row in corpus:\n","        qs = []\n","        for word in row.split():\n","            if word not in keep_list:\n","                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n","                p1 = p1.lower()\n","                qs.append(p1)\n","            else : qs.append(word)\n","        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n","    return cleaned_corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_FV7GnxYBHW"},"source":["def stopwords_removal(corpus):\n","    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n","    stop = set(stopwords.words('english'))\n","    for word in wh_words:\n","        stop.remove(word)\n","    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zYlP7s27YBHW"},"source":["def lemmatize(corpus):\n","    lem = WordNetLemmatizer()\n","    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PxSwnQ1RYBHX"},"source":["def stem(corpus, stem_type = None):\n","    if stem_type == 'snowball':\n","        stemmer = SnowballStemmer(language = 'english')\n","        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n","    else :\n","        stemmer = PorterStemmer()\n","        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiS3iWnhYBHX"},"source":["def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n","    '''\n","    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n","    \n","    Input : \n","    'corpus' - Text corpus on which pre-processing tasks will be performed\n","    'keep_list' - List of words to be retained during cleaning process\n","    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should \n","                                                                  be performed or not\n","    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n","                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n","    \n","    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n","    \n","    Output : Returns the processed text corpus\n","    \n","    '''\n","    \n","    if cleaning == True:\n","        corpus = text_clean(corpus, keep_list)\n","    \n","    if remove_stopwords == True:\n","        corpus = stopwords_removal(corpus)\n","    else :\n","        corpus = [[x for x in x.split()] for x in corpus]\n","    \n","    if lemmatization == True:\n","        corpus = lemmatize(corpus)\n","        \n","        \n","    if stemming == True:\n","        corpus = stem(corpus, stem_type)\n","    \n","    corpus = [' '.join(x) for x in corpus]        \n","\n","    return corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMybcwELYBHY"},"source":["common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6U_s6g3nwTvO"},"source":["# Preprocessing with Lemmatization here\n","preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None,\n","                                lemmatization = True, remove_stopwords = True)\n","preprocessed_corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CLRnxWfE2JWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4cDej96YYBHZ"},"source":["## Building the vocabulary"]},{"cell_type":"code","metadata":{"id":"lm7c2j9sYBHZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634790671584,"user_tz":-420,"elapsed":359,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"09edc00f-f86b-4127-eb02-1577f7d14eb2"},"source":["set_of_words = set()\n","for sentence in preprocessed_corpus:\n","    for word in sentence.split():\n","        set_of_words.add(word)\n","vocab = list(set_of_words)\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['make', 'process', 'comprehend', 'everyday', 'language', 'evolve', 'data', 'computers', 'natural', 'field', 'read']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Q7Jz6VmW2MJi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmFYPkwfYBHa"},"source":["## Fetching the position of each word in the vocabulary"]},{"cell_type":"code","metadata":{"id":"cPxs17E6YBHa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634790681456,"user_tz":-420,"elapsed":339,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"e1f14053-e71c-48e9-b215-905afd455acd"},"source":["position = {}\n","for i, token in enumerate(vocab):\n","    position[token] = i\n","print(position)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'make': 0, 'process': 1, 'comprehend': 2, 'everyday': 3, 'language': 4, 'evolve': 5, 'data': 6, 'computers': 7, 'natural': 8, 'field': 9, 'read': 10}\n"]}]},{"cell_type":"markdown","metadata":{"id":"WKra5kXSYBHa"},"source":["## Creating a matrix to hold the Bag of Words representation"]},{"cell_type":"code","metadata":{"id":"bJnh-YyfYBHb","executionInfo":{"status":"error","timestamp":1666184501961,"user_tz":-60,"elapsed":35,"user":{"displayName":"ekawulan yunita","userId":"17180113555516654015"}},"outputId":"1ccc198f-962c-49db-a3c6-aedd115aa0d0","colab":{"base_uri":"https://localhost:8080/","height":172}},"source":["bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-675b9b4ec9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbow_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"code","metadata":{"id":"EKZC9gP6YBHb"},"source":["for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n","    for token in preprocessed_sentence.split():   \n","        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWnc7fdLYBHb"},"source":["## Let's look at our Bag of Words representation"]},{"cell_type":"code","metadata":{"id":"NMd49chnYBHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634790722879,"user_tz":-420,"elapsed":529,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"603513d9-cf24-425b-9d84-288f3efaf7b5"},"source":["bow_matrix"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1.],\n","       [1., 1., 1., 0., 2., 0., 1., 1., 1., 0., 0.],\n","       [0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0.]])"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"2DdsMpQhSI-A"},"source":["Inference\n","\n","Taking example of column 2 in the bow_matrix, the values are 1, 2 and 1 respectively.\n","\n","Column 2 caters to index 2 corresponding to the word *language*.\n","\n","*language* occurs **once, twice and again once** in the the sentences 1, 2 and 3 respectively.\n","\n","Hope that provides you insights into how the Bag of Words model works."]},{"cell_type":"markdown","metadata":{"id":"nd4-g5kqRjUa"},"source":["#N-Gram"]},{"cell_type":"markdown","metadata":{"id":"pT0zLV3qRpBH"},"source":["## Let's see how can bigrams and trigrams can be included here"]},{"cell_type":"code","metadata":{"id":"ph3ZTyQqg3od","executionInfo":{"status":"error","timestamp":1666191316447,"user_tz":-420,"elapsed":8,"user":{"displayName":"Muhamad Rifki Arisagas","userId":"05734629327720317968"}},"outputId":"8982b789-e4cc-4596-d185-f82eeec7740a","colab":{"base_uri":"https://localhost:8080/","height":183}},"source":["vectorizer = CountVectorizer()\n","bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6587198091c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbow_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"OMP-8t39R8Du"},"source":["vectorizer_ngram_range = CountVectorizer(analyzer='word', ngram_range=(1,3))\n","bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swj3HoJLSAjM"},"source":["print(vectorizer_ngram_range.get_feature_names())\n","print(bow_matrix_ngram.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oJYTZrUJSOnI"},"source":["Inference\n","As can be seen, the 9th phrase from the end *natural language process* occurs once in every sentence.\n","\n","The column corresponding to it has the entries **1, 1 and 1**."]},{"cell_type":"markdown","metadata":{"id":"KeJP2NvdjYgp"},"source":["## Max_feature"]},{"cell_type":"code","metadata":{"id":"VoVNh1gyjdtg"},"source":["vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_features = 6)\n","bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AkWo-6njrKp"},"source":["print(vectorizer_max_features.get_feature_names())\n","print(bow_matrix_max_features.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PqYU0rJam-ZR"},"source":["## Max_df - Min_df"]},{"cell_type":"code","metadata":{"id":"aYlidLQ_nDPE"},"source":["vectorizer_max_min = CountVectorizer(analyzer='word', ngram_range=(1,3), max_df =3, min_df = 2)\n","bow_matrix_max_min = vectorizer_max_min.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39LFH1RunI0J"},"source":["print(vectorizer_max_min.get_feature_names())\n","print(bow_matrix_max_min.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zuBnq6XaZvYt"},"source":["#TF-IDF"]},{"cell_type":"markdown","metadata":{"id":"2wH1drY2Zz1A"},"source":["##Building TF-IDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"equS5I2HaM0L"},"source":["vectorizer = TfidfVectorizer()\n","tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ml4hgVP_aTXN"},"source":["##Let's what features were obtained and the corresponding TF-IDF matrix"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5hvO9leaZ3h","executionInfo":{"status":"ok","timestamp":1634779080365,"user_tz":-420,"elapsed":355,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"8830e232-7b42-447e-8ebd-a2ca3bbc4849"},"source":["print(vectorizer.get_feature_names())\n","print(tf_idf_matrix.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0.         0.         0.         0.         0.         0.\n","  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n"," [0.40512186 0.40512186 0.40512186 0.         0.         0.\n","  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n"," [0.         0.         0.         0.49711994 0.49711994 0.49711994\n","  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n","\n","The shape of the TF-IDF matrix is:  (3, 11)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ex-GsqOaaqAF"},"source":["## Changing the norm to l1, default option is l2 which was used above"]},{"cell_type":"code","metadata":{"id":"pad3Bq3Oask2"},"source":["vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n","tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-N0rFdgda1-K","executionInfo":{"status":"ok","timestamp":1634779085084,"user_tz":-420,"elapsed":543,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"c9817cbf-1577-45fc-ef11-7158a537c029"},"source":["print(vectorizer_l1_norm.get_feature_names())\n","print(tf_idf_matrix_l1_norm.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['comprehend', 'computers', 'data', 'everyday', 'evolve', 'field', 'language', 'make', 'natural', 'process', 'read']\n","[[0.         0.         0.         0.         0.         0.\n","  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n"," [0.1571718  0.1571718  0.1571718  0.         0.         0.\n","  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n"," [0.         0.         0.         0.2095624  0.2095624  0.2095624\n","  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n","\n","The shape of the TF-IDF matrix is:  (3, 11)\n"]}]},{"cell_type":"markdown","metadata":{"id":"x016FrZzbFGa"},"source":["##N-grams and Max features with TfidfVectorizer"]},{"cell_type":"code","metadata":{"id":"VfFFVU59bI0U"},"source":["vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\", analyzer='word', ngram_range=(1,3), max_features = 6)\n","tf_idf_matrix_n_gram_max_features = vectorizer_n_gram_max_features.fit_transform(preprocessed_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxHOENCAbQTE","executionInfo":{"status":"ok","timestamp":1634779090318,"user_tz":-420,"elapsed":340,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"b2852bea-165c-4e46-c5e8-d30c7a42b33f"},"source":["print(vectorizer_n_gram_max_features.get_feature_names())\n","print(tf_idf_matrix_n_gram_max_features.toarray())\n","print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_n_gram_max_features.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['language', 'language process', 'natural', 'natural language', 'natural language process', 'process']\n","[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n"," [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n"," [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n","\n","The shape of the TF-IDF matrix is:  (3, 6)\n"]}]},{"cell_type":"markdown","metadata":{"id":"qS-DZyceaGgz"},"source":["#Cosine Similarity"]},{"cell_type":"markdown","metadata":{"id":"DvhXPPXkaOZz"},"source":["##Cosine Similarity"]},{"cell_type":"code","metadata":{"id":"pJ3OA5GhaYrC"},"source":["def cosine_similarity(vector1, vector2):\n","    vector1 = np.array(vector1)\n","    vector2 = np.array(vector2)\n","    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ai98TF_wa48O"},"source":["## Cosine similarity between the document vectors built using CountVectorizer (BoW-Basic)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iEJFi5qXav68","executionInfo":{"status":"ok","timestamp":1634779096373,"user_tz":-420,"elapsed":4,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"cd983a85-0990-4a28-943a-1d4413332104"},"source":["for i in range(bow_matrix.shape[0]):\n","    for j in range(i + 1, bow_matrix.shape[0]):\n","        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n","              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n","The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n","The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"]}]},{"cell_type":"markdown","metadata":{"id":"sC8Kc8b0b60z"},"source":["##Cosine similarity between the document vectors built using TfidfVectorizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wg150UnncLiq","executionInfo":{"status":"ok","timestamp":1634779342197,"user_tz":-420,"elapsed":424,"user":{"displayName":"DWI INTAN AF'IDAH","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRi_7CKiYyHzIfY6nkuUKXXw2EVvS5n-Hsr6Lu=s64","userId":"02413713999392776136"}},"outputId":"6e8b22d5-78f6-47a7-b657-ae1d11cbfa3f"},"source":["for i in range(tf_idf_matrix.shape[0]):\n","    for j in range(i + 1, tf_idf_matrix.shape[0]):\n","        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n","              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n","The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n","The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"]}]}]}